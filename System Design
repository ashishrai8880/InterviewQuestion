1. What is difference between Sharding and Partitioning ?
Sharding me db ko seperate krte hai for different region of users . Means us , uk , japan wale users k liye unka db hoga jo ki us region k close hoga . 
Partioning me jabki , same database ko split krte hai , means agar product table me 1 million of product hai to usko 4 small db me save kr sakte hai 255k data in 
each small db . 
Sharding me koi route setup krna padta hai , jisse pta chale request kon se region se hai .

In simple terms, both sharding and partitioning are ways of dividing a database into smaller, more manageable pieces, but they differ in how and why they do this.

1. Sharding:
Sharding involves splitting a database into multiple independent databases or "shards," each of which holds a subset of the data. These shards are usually distributed across different servers (or machines), often in different physical locations.

Example: Imagine you have a large e-commerce database with customer data. If the database is getting too large and slow, you can shard the data by splitting it into different shards based on regions.
Shard 1: Customers from the U.S.
Shard 2: Customers from Europe
Shard 3: Customers from Asia Each shard can be stored on a different server, making the database faster and more efficient.
Key Point: Sharding is often used when a database grows so large that a single server can't handle the load, and you need to spread the data across multiple machines.

2. Partitioning:
Partitioning is the process of dividing a single database into smaller pieces called partitions. Each partition holds a part of the data, but all partitions still exist in the same database and typically on the same server.

Example: Imagine you have a database of sales transactions. You can partition the data by year:
Partition 1: Sales data from 2020
Partition 2: Sales data from 2021
Partition 3: Sales data from 2022 All the data is still in the same database, but each year’s data is in its own partition, making queries faster by limiting how much data is processed at once.
Key Point: Partitioning is often done to make large tables easier to manage or query, but all the partitions typically stay on the same server.

Key Differences:
Sharding: Splits the data into separate databases on different servers.
Used for scalability when a single server can't handle the load.
Partitioning: Splits the data into smaller pieces (partitions), but all pieces stay in the same database (usually on the same server).
Used to organize large datasets for better management or performance within a single database.
In Summary:
Sharding = Splitting across different databases/servers.
Partitioning = Splitting within the same database/server.


***********************************************************************************************************************************************************
***********************************************************************************************************************************************************

2. What is CAP Theorem ?
Ans :  LINK ->  https://drive.google.com/drive/folders/1i80S1YymUX8pWId6tAnh10rS2FeO-Mbb

CAP theorem means consistency , availabiliy and partioning tolerance me se koi 2 achieve kr sakte hai in distributed system .
The CAP Theorem is a concept from computer science that applies to distributed systems (systems where data is spread across multiple machines or locations). It states that in any distributed system, you can only guarantee two out of three of the following properties at the same time:

Consistency (C)
Availability (A)
Partition tolerance (P)
Let’s break it down:

1. Consistency (C):
Every request to the database will return the latest data, meaning that after an update, all nodes (servers) in the system will have the same data at the same time. No matter which server you ask, you’ll get the same result.

Example: If you're checking your bank balance from two different devices, Consistency means that both devices should show the exact same balance at the same time.
2. Availability (A):
Every request will receive a response, either with data or an error message. The system remains available, even if some of the nodes (servers) are down or unreachable.

Example: If you're trying to access a website, Availability means that the system will always respond, even if some of its parts (servers) are temporarily unavailable.
3. Partition Tolerance (P):
The system will continue to operate, even if there is a network failure that prevents some parts of the system from communicating with each other. This means the system can still work even if some servers can’t reach others due to network issues.

Example: Imagine you are in a video call, and due to network issues, some participants can’t hear each other. Partition tolerance ensures that the call can continue for those who are still connected, even if some users are temporarily cut off.
The CAP Theorem’s Core Idea:
A distributed system cannot guarantee all three of these properties (Consistency, Availability, and Partition Tolerance) at the same time.
Instead, it can guarantee at most two of them, and the system must choose which two to prioritize based on the specific needs.
The Three Possible Choices:
Consistency + Availability (CA): You get the most up-to-date data, and the system is always available, but if there’s a network partition, some parts of the system might not be reachable.

Example: A system where you can always access your data (availability), and you’ll always see the latest data (consistency), but it struggles if there’s a network failure between servers.
Consistency + Partition Tolerance (CP): The system ensures that all nodes have the same data (consistency) and will keep working even if there's a network partition. However, it may not always be available because it might stop answering requests if there's an inconsistency.

Example: A database where you might experience downtime (no response) during network failures, but when it does respond, you'll always get the most up-to-date data.
Availability + Partition Tolerance (AP): The system remains available (always responds), and will continue to work through network partitions, but may not always give the most up-to-date data (some nodes might have old data).

Example: A system where, during network issues, you can still access data, but it might be out-of-sync across different servers.
In Summary:
The CAP Theorem tells us that in a distributed system, you can't have all three of the following at the same time:

Consistency (latest data for everyone),
Availability (always responds),
Partition tolerance (works even if parts of the system are disconnected).
Instead, you can only choose two to prioritize depending on the system's needs.


***********************************************************************************************************************************************************
***********************************************************************************************************************************************************

3. What are caching and what are different cache update strategies ?
Caching is a mechanism used to store frequently accessed data temporarily in a fast storage layer (like memory) to improve performance and reduce the time taken to retrieve data from a slower storage layer (like a database or external API). The goal is to serve requests faster by reducing the load on the primary data source.

Cache Update Strategies
Several strategies are used to manage and update cached data to ensure it remains relevant and useful. Here are the most common ones:

1. Write-Through Cache
Description:
Every time data is written to the database, it is simultaneously written to the cache. This ensures that the cache and the database are always in sync.

Example:

Suppose you're managing a user profile system.
When a user updates their profile, the system writes the updated data to the database and immediately updates the cache.
Advantages:

Data consistency between the cache and database.
Read requests can always fetch fresh data from the cache.
Disadvantage:

Slower writes because it involves updating both the database and cache.
2. Write-Around Cache
Description:
Data is written directly to the database, and the cache is updated only when the data is read the next time (cache miss).

Example:

A blog application writes a new blog post directly to the database.
The blog post will not appear in the cache until someone reads it for the first time.
Advantages:

Faster writes since the cache is not updated.
Useful when the data is rarely read.
Disadvantages:

May result in initial cache misses for newly written data.
3. Write-Back Cache
Description:
Data is written only to the cache initially. The database is updated asynchronously in the background. This can improve performance but requires a mechanism to ensure eventual consistency.

Example:

A shopping cart system updates the cache when a user adds items to their cart. The database is updated in bulk periodically.
Advantages:

Very fast writes since only the cache is updated.
Reduces database write load.
Disadvantages:

Risk of data loss if the cache crashes before the database is updated.
Complex implementation to ensure consistency.
4. Cache-Aside (Lazy Loading)
Description:
The application checks the cache first. If the data is not found (cache miss), it fetches the data from the database and then populates the cache for future requests.

Example:

An e-commerce site stores product details in the cache.
When a user views a product for the first time, it fetches the details from the database, stores them in the cache, and serves the response.
Advantages:

Cache only contains frequently accessed data, optimizing memory usage.
Simpler implementation.
Disadvantages:

Cache misses can result in slower initial response times.
Stale data if not properly managed.
5. Refresh-Ahead
Description:
The cache predicts when data will be accessed and refreshes it proactively before it is requested, ensuring fresh data is always available.

Example:

A news application knows that the "Top Stories" page is frequently accessed.
It refreshes the cache every 10 minutes to pre-load the latest stories.
Advantages:

Reduces cache misses.
Ensures fresh data is always served.
Disadvantages:

May refresh data unnecessarily, leading to higher costs.
Complex to predict user access patterns accurately.
Choosing the Right Strategy
The choice depends on the specific requirements of your application:

Write-heavy applications: Use Write-Back or Write-Around.
Read-heavy applications: Use Write-Through or Cache-Aside.
Real-time applications: Consider Refresh-Ahead.
Example Scenario
Imagine an online food delivery system:

User views restaurant menu: Cache-Aside is used. If the menu is not in the cache, fetch it from the database and store it in the cache.
User places an order: Write-Through ensures the order details are updated in both the cache and the database simultaneously.
Frequent updates for popular restaurants: Refresh-Ahead preloads the menu periodically to avoid delays during peak times.
By choosing the right strategy, you can balance performance, consistency, and system complexity.
