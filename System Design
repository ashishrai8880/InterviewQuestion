1. What is difference between Sharding and Partitioning ?
Sharding me db ko seperate krte hai for different region of users . Means us , uk , japan wale users k liye unka db hoga jo ki us region k close hoga . 
Partioning me jabki , same database ko split krte hai , means agar product table me 1 million of product hai to usko 4 small db me save kr sakte hai 255k data in 
each small db . 
Sharding me koi route setup krna padta hai , jisse pta chale request kon se region se hai .

In simple terms, both sharding and partitioning are ways of dividing a database into smaller, more manageable pieces, but they differ in how and why they do this.

1. Sharding:
Sharding involves splitting a database into multiple independent databases or "shards," each of which holds a subset of the data. These shards are usually distributed across different servers (or machines), often in different physical locations.

Example: Imagine you have a large e-commerce database with customer data. If the database is getting too large and slow, you can shard the data by splitting it into different shards based on regions.
Shard 1: Customers from the U.S.
Shard 2: Customers from Europe
Shard 3: Customers from Asia Each shard can be stored on a different server, making the database faster and more efficient.
Key Point: Sharding is often used when a database grows so large that a single server can't handle the load, and you need to spread the data across multiple machines.

2. Partitioning:
Partitioning is the process of dividing a single database into smaller pieces called partitions. Each partition holds a part of the data, but all partitions still exist in the same database and typically on the same server.

Example: Imagine you have a database of sales transactions. You can partition the data by year:
Partition 1: Sales data from 2020
Partition 2: Sales data from 2021
Partition 3: Sales data from 2022 All the data is still in the same database, but each year’s data is in its own partition, making queries faster by limiting how much data is processed at once.
Key Point: Partitioning is often done to make large tables easier to manage or query, but all the partitions typically stay on the same server.

Key Differences:
Sharding: Splits the data into separate databases on different servers.
Used for scalability when a single server can't handle the load.
Partitioning: Splits the data into smaller pieces (partitions), but all pieces stay in the same database (usually on the same server).
Used to organize large datasets for better management or performance within a single database.
In Summary:
Sharding = Splitting across different databases/servers.
Partitioning = Splitting within the same database/server.


***********************************************************************************************************************************************************
***********************************************************************************************************************************************************

2. What is CAP Theorem ?
Ans :  LINK ->  https://drive.google.com/drive/folders/1i80S1YymUX8pWId6tAnh10rS2FeO-Mbb

CAP theorem means consistency , availabiliy and partioning tolerance me se koi 2 achieve kr sakte hai in distributed system .
The CAP Theorem is a concept from computer science that applies to distributed systems (systems where data is spread across multiple machines or locations). It states that in any distributed system, you can only guarantee two out of three of the following properties at the same time:

Consistency (C)
Availability (A)
Partition tolerance (P)
Let’s break it down:

1. Consistency (C):
Every request to the database will return the latest data, meaning that after an update, all nodes (servers) in the system will have the same data at the same time. No matter which server you ask, you’ll get the same result.

Example: If you're checking your bank balance from two different devices, Consistency means that both devices should show the exact same balance at the same time.
2. Availability (A):
Every request will receive a response, either with data or an error message. The system remains available, even if some of the nodes (servers) are down or unreachable.

Example: If you're trying to access a website, Availability means that the system will always respond, even if some of its parts (servers) are temporarily unavailable.
3. Partition Tolerance (P):
The system will continue to operate, even if there is a network failure that prevents some parts of the system from communicating with each other. This means the system can still work even if some servers can’t reach others due to network issues.

Example: Imagine you are in a video call, and due to network issues, some participants can’t hear each other. Partition tolerance ensures that the call can continue for those who are still connected, even if some users are temporarily cut off.
The CAP Theorem’s Core Idea:
A distributed system cannot guarantee all three of these properties (Consistency, Availability, and Partition Tolerance) at the same time.
Instead, it can guarantee at most two of them, and the system must choose which two to prioritize based on the specific needs.
The Three Possible Choices:
Consistency + Availability (CA): You get the most up-to-date data, and the system is always available, but if there’s a network partition, some parts of the system might not be reachable.

Example: A system where you can always access your data (availability), and you’ll always see the latest data (consistency), but it struggles if there’s a network failure between servers.
Consistency + Partition Tolerance (CP): The system ensures that all nodes have the same data (consistency) and will keep working even if there's a network partition. However, it may not always be available because it might stop answering requests if there's an inconsistency.

Example: A database where you might experience downtime (no response) during network failures, but when it does respond, you'll always get the most up-to-date data.
Availability + Partition Tolerance (AP): The system remains available (always responds), and will continue to work through network partitions, but may not always give the most up-to-date data (some nodes might have old data).

Example: A system where, during network issues, you can still access data, but it might be out-of-sync across different servers.
In Summary:
The CAP Theorem tells us that in a distributed system, you can't have all three of the following at the same time:

Consistency (latest data for everyone),
Availability (always responds),
Partition tolerance (works even if parts of the system are disconnected).
Instead, you can only choose two to prioritize depending on the system's needs.


***********************************************************************************************************************************************************
***********************************************************************************************************************************************************

3. What are caching and what are different cache update strategies ?
Caching is a mechanism used to store frequently accessed data temporarily in a fast storage layer (like memory) to improve performance and reduce the time taken to retrieve data from a slower storage layer (like a database or external API). The goal is to serve requests faster by reducing the load on the primary data source.

Cache Update Strategies
Several strategies are used to manage and update cached data to ensure it remains relevant and useful. Here are the most common ones:

1. Write-Through Cache
Description:
Every time data is written to the database, it is simultaneously written to the cache. This ensures that the cache and the database are always in sync.

Example:

Suppose you're managing a user profile system.
When a user updates their profile, the system writes the updated data to the database and immediately updates the cache.
Advantages:

Data consistency between the cache and database.
Read requests can always fetch fresh data from the cache.
Disadvantage:

Slower writes because it involves updating both the database and cache.
2. Write-Around Cache
Description:
Data is written directly to the database, and the cache is updated only when the data is read the next time (cache miss).

Example:

A blog application writes a new blog post directly to the database.
The blog post will not appear in the cache until someone reads it for the first time.
Advantages:

Faster writes since the cache is not updated.
Useful when the data is rarely read.
Disadvantages:

May result in initial cache misses for newly written data.
3. Write-Back Cache
Description:
Data is written only to the cache initially. The database is updated asynchronously in the background. This can improve performance but requires a mechanism to ensure eventual consistency.

Example:

A shopping cart system updates the cache when a user adds items to their cart. The database is updated in bulk periodically.
Advantages:

Very fast writes since only the cache is updated.
Reduces database write load.
Disadvantages:

Risk of data loss if the cache crashes before the database is updated.
Complex implementation to ensure consistency.
4. Cache-Aside (Lazy Loading)
Description:
The application checks the cache first. If the data is not found (cache miss), it fetches the data from the database and then populates the cache for future requests.

Example:

An e-commerce site stores product details in the cache.
When a user views a product for the first time, it fetches the details from the database, stores them in the cache, and serves the response.
Advantages:

Cache only contains frequently accessed data, optimizing memory usage.
Simpler implementation.
Disadvantages:

Cache misses can result in slower initial response times.
Stale data if not properly managed.
5. Refresh-Ahead
Description:
The cache predicts when data will be accessed and refreshes it proactively before it is requested, ensuring fresh data is always available.

Example:

A news application knows that the "Top Stories" page is frequently accessed.
It refreshes the cache every 10 minutes to pre-load the latest stories.
Advantages:

Reduces cache misses.
Ensures fresh data is always served.
Disadvantages:

May refresh data unnecessarily, leading to higher costs.
Complex to predict user access patterns accurately.
Choosing the Right Strategy
The choice depends on the specific requirements of your application:

Write-heavy applications: Use Write-Back or Write-Around.
Read-heavy applications: Use Write-Through or Cache-Aside.
Real-time applications: Consider Refresh-Ahead.
Example Scenario
Imagine an online food delivery system:

User views restaurant menu: Cache-Aside is used. If the menu is not in the cache, fetch it from the database and store it in the cache.
User places an order: Write-Through ensures the order details are updated in both the cache and the database simultaneously.
Frequent updates for popular restaurants: Refresh-Ahead preloads the menu periodically to avoid delays during peak times.
By choosing the right strategy, you can balance performance, consistency, and system complexity.


***********************************************************************************************************************************************************
***********************************************************************************************************************************************************

4. When someone hit any domain like facebook.com , then what will happen?

Ans: 1. First it checks in the local browser cache.
 If in cache it is not present, to uske baad ye "root server " k pass jata hai . root server like .com .in .edu etc . 
humesha right to left url parse hota hai
 . root server k baad ye llike .com k niche "Authorative server" k pass jata hai , or waha pe isko us domain name ka ip 
milta hai and then fir resolver us ip 
ko leke us ip address k server se data request krta hai . 


***********************************************************************************************************************************************************
***********************************************************************************************************************************************************

5. What is SOLID principle ?
The SOLID principles are five essential guidelines that enhance software design, making code more maintainable and scalable.

These five principles are:

Single Responsibility Principle (SRP)
Open/Closed Principle
Liskov’s Substitution Principle (LSP)
Interface Segregation Principle (ISP)
Dependency Inversion Principle (DIP)

1. Single Responsibility Principle : This principle states that “A class should have only one reason to change” which
means every class should have a single responsibility or single job or single purpose. In other words, a class should
have only one job or purpose within the software system.

2. Open/Closed Principle : This principle states that “Software entities (classes, modules, functions, etc.) should be
open for extension, but closed for modification” which means you should be able to extend a class behavior, without
modifying it.

 Imagine you have a class called PaymentProcessor that processes payments for an online store. Initially, the
PaymentProcessor class only supports processing payments using credit cards. However, you want to extend its
functionality to also support processing payments using PayPal.

Instead of modifying the existing PaymentProcessor class to add PayPal support, you can create a new class called
PayPalPaymentProcessor that extends the PaymentProcessor class. This way, the PaymentProcessor class remains closed
for modification but open for extension, adhering to the Open-Closed Principle.

3. Liskov’s Substitution Principle :  The principle was introduced by Barbara Liskov in 1987 and according to this
principle “Derived or child classes must be substitutable for their base or parent classes“. This principle ensures
that any class that is the child of a parent class should be usable in place of its parent without any unexpected behavior.

If class B is a subtype of class A , then we should be able to replace Object A with Object B , without breaking the behavior
of program . 

interface Bike {
  public turnOnEngine(){this.isEngineOn = true};

  public increaseSpeed() {this.speed += 10 } ;
}

class MotorCycle implements Bike{
   public turnOnEngine() {
     throw new Exception ("exception");
   }
public increaseSpeed() {
     throw new Exception ("exception");
   }
}


class BiCycle implements Bike{
   public turnOnEngine() {
     throw new Exception ("exception");
   }
public increaseSpeed() {
     throw new Exception ("exception");
   }
} .

Now suppose if somewhere Object of Bike is using and when we replace Object of Bike with Object of MotorCycle then 
it should not create an error in program . It should work properly . We can also say that , child class should 
contain all property of parent class , child class should not degrade parent class property .

4. Interface Segregation Principle :  It states that “do not force any client to implement an interface which is 
irrelevant to them“.

5. Dependency Inversion Principle : The Dependency Inversion Principle (DIP) is a principle in object-oriented design
that states that “High-level modules should not depend on low-level modules. Both should depend on abstractions“.
We can also say that , "class should depend on interface rather than concret class " .
