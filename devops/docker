Docker Notes

->  Docker is an open-source containerization platform by which you can pack your application and all its dependencies into a standardized unit called a container.
Containers are light in weight, which makes them portable, and they are isolated from the underlying infrastructure and from each other's container.
You can run the docker image as a docker container in any machine where docker is installed without depending on the operating system.

->Docker Image is an executable package of software that includes everything needed to run an application. This image informs how a container should instantiate, determining which software components will run and how.
Docker container is a runtime instance of an image. Allows developers to package applications with all parts needed such as libraries and other dependencies. Docker Containers are runtime instances of Docker images.

===================================================> Architecture of Docker 
Docker makes use of a client-server architecture. The Docker client talks with the docker daemon which helps in building, running, and distributing the docker containers. The Docker client runs with the daemon on the 
same system or we can connect the Docker client with the Docker daemon remotely. With the help of REST API over a  UNIX socket or a network, the docker client and daemon interact with each other. 

Docker daemon manages all the services by communicating with other daemons. It manages docker objects such as images, containers, networks, and volumes with the help of the API requests of Docker.

Docker Client
With the help of the docker client, the docker users can interact with the docker. The docker command uses the Docker API. The Docker client can communicate with multiple daemons. When a docker 
client runs any docker command on the docker terminal then the terminal sends instructions to the daemon. The Docker daemon gets those instructions from the docker client withinside the shape of 
the command and REST API's request

Docker Host
A Docker host is a type of machine that is responsible for running more than one container. It comprises the Docker daemon, Images, Containers, Networks, and Storage.

Docker Registry
All the docker images are stored in the docker registry. There is a public registry which is known as a docker hub that can be used by anyone. We can run our private 
registry also. With the help of docker run or docker pull commands, we can pull the required images from our configured registry. Images are pushed into configured registry 
with the help of the docker push command.

Docker Engine:

The Docker Engine is the heart of the Docker platform. It comprises two main components:
Docker Daemon (dockerd): The Docker daemon runs on the host machine and is responsible for managing Docker objects, such as images, containers, networks, and volumes.
Docker Client: The Docker client is a command-line interface (CLI) tool that allows users to interact with the Docker daemon through commands. Users can build, run, stop,
and manage Docker containers using the Docker CLI.

Docker Container Lifecycle Management
The lifecycle of a docker container consists of five states:

Created state
Running state
Paused state/unpaused state
Stopped state
Killed/Deleted state


=====================================================================Commands =============================================================================

  docker ps : list all running containers
  docker run -d <imagename>  : run docker imae in detach mode . detach mode means it can run in backgroud . 
  docker stop <container id> : used to stop docker container 

FROM openjdk:17-jdk-alpine        //  openjdk is base image . 17 is the version of jdk and -alpine is the tag .

WORKDIR /app             // defining work directory . whatever command will run will run inside this directory

COPY src/Main.java /app/Main.java                // copy specific file from host directory to working container directory . This is basic project , that is why i copied only specific file , otherwise entire project should be copied

RUN javac Main.java               // compiling java program

CMD ["java","Main"]                      // running the container 


Every line in Dockerfile , is used to make image of project . All commmand till RUN is used to make container . if i do not write CMD command , still image will be built . To run built image in container , 
we need to write CMD command . This is basic difference between RUN and CMD command . RUN is used to build image and CMD is used to actually runn the container (from image) . 
There is similar ENTRYPOINT command as same as CMD . difference is CMD be be overriden and ENTRYPOINT can not be overwridden . 

  docker build -t <new image name>  .    : this command is used to build the image of project . -t is used to give image tag name . and (.) this is used to tell docker build command , where is exact Dockerfile present . 
                                            docker build command build image from Dockerfile . So we need to tell the location of Dockerfile in a project . 

  docker run -d <image tag name>      : this command is used to run container . -d used to run in detach mode (backgroud mode) .


iF ANYthing changes in project then we need to run docker build command again to make fresh images . and then again need to run docker run command to run docker container . 

============================================================= Running Flask Project =================================================================================

  docker run -d -p 80:80 <image-name>   :  here -p used to public host port with container port . first 80 is the host port and second 80 is the container port . It basically telling , public container 80 port to the host
                                            80 port . 

  docker logs <container id >  : To get project logs . It can even give which endpoint is hit . 

  docker attach <container id>  : It will also give project logs , but in real time in attach mode . docker logs command can not give me real time . 

  docker start <container id> : to start container 
  docker stop <container id > : to stop container 

  docker exec -it <containerId> bash   :  -it : iteractive . this command used to go inside container . like if we run mysql container , then to go inside we can use this command . exec is executes . bash is because
                                            it is command line . 

  docker images : it shows all the images . 

  docker run -itd ubuntu : this command will run ubuntu in backgroud + in iteractive mode . This will give me ubuntu terminal . -it : interactive terminal and -d : detach mode 


====================================================================== Docker Networking =============================================================================================

Docker Networking 
A network is a group of two or more devices that can communicate with each other either physically or virtually. The Docker network is a virtual network created by Docker to enable 
communication between Docker containers. If two containers are running on the same host they can communicate with each other without the need for ports to be exposed to the host machine.

Types of Network Drivers
bridge (default): If you build a container without specifying the kind of driver, the container will only be created in the bridge network, which is the default network. 

Custom ( user defined bridge ) : This is defined by user . 

host: Containers will not have any IP address they will be directly created in the system network which will remove isolation between the docker host and containers. container and host both will have same port and network .

none: IP addresses won't be assigned to containers. These containments are not accessible to us from the outside or from any other container. 

overlay: overlay network will enable the connection between multiple Docker demons and make different Docker swarm services communicate with each other.

ipvlan: Users have complete control over both IPv4 and IPv6 addressing by using the IPvlan driver.

macvlan: macvlan driver makes it possible to assign MAC addresses to a container. 

  1. docker network ls : to view all network available

  2. docker network create mynetwork -d bridge : use to create new network

  3. docker run -e MYSQL_ROOT_PASSWORD=root mysql : -e used to give environement . mysql cant run without environment . 

  4. docker run -d -p 5000:5000 -e MYSQL_HOST=mysql -e MYSQL_USER=root -e MYSQL_PASSWORD=root -e MYSQL_DB=devops two-tier-backend

  5. docker ps -a : give all container , even past container also . 

  6. docker stop <cID> & docker rm <cId> : stop container and remove it .

  7. docker run -d --name mysql-container --network <networkname> -e MYSQL_ROOT_PASSWORD=root -e MYSQL_DATABASE=devops mysql . To run mysql in particular network . 

If any app contains db and other requirements , then we need to run all these dependency in same network . For example , if app is using mysql db , then first create
network of bridge type and then run mysql and application in same network then only it will work . otherwise app will be unable to find
database host . 

  docker run -d -p 5000:5000 --network <network-name> -e MYSQL_HOST=<mysql container name> -e MYSQL_USER=root -e MYSQL_PASSWORD=root -e MYSQL_DB=devops --name <app-container-name> <image-name>   : This command can be used to 
    run application within same network . 

  8. docker network inspect <network-name> : use to inspect network 

  9. docker exec -itd <containerId> bash : used to enter into mysql command prompt


================================================Docker Volume ====================================================

Volumes are persistent data stores for containers, created and managed by Docker. You can create a volume explicitly using the
docker volume create command, or Docker can create a volume during container or service creation.

  1. docker volume ls : list all volume
  2. docker volume create mysql-data  : used to create volume . mysql-data is the name of volume . 
  3. docker inspect <volume-name> : you can watch where it is storing , what data it holds . 

  4. docker run -d --name mysql-container --network <networkname> -v mysql-data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=root -e MYSQL_DATABASE=devops mysql . : in this command attaching volume with mysql container . /var/lib/mysql is actual place in container 
                                where data is actually saved . And it is mapped with the host . so that data will not lost . 

===============================================Docker Compose====================================================

Docker Compose is a powerful tool for managing multi-container applications, and mastering its key components—like services, 
networks, volumes, and environment variables—can greatly enhance its usage. Let’s break down these concepts and how they work 
within a Docker Compose file.

Docker Compose File (YAML Format)
Docker Compose configurations are mainly stored in a file named docker-compose.yml, which uses YAML format to define an 
application’s environment. This file includes all the necessary details to set up and run your application, such as services,
networks, and volumes. To use Docker Compose in an effective way you have to know the structure of this file.

Before docker compose , we are first making image by running docker build command , then we run container by docker run command . 
In running mysql container , we also have to pass environment variables each time which is time consuming process . Instead doing all this configuration
in terminal every time , we can simply write all this configuration in docker-compose yaml file and automate process . Soundss Cool . 

version: "3.8"     // docker compose version . nowadays , it is optional

services:
  mysql:          // service name 
    user: "${UID}:${GID}" 
    image: mysql:5.7           // image : because it should be pull from registory
    container_name: mysql
    environment:
      MYSQL_ROOT_PASSWORD: root     // environement variable which is required to connect mysql
      MYSQL_DATABASE: devops
      MYSQL_USER: admin
      MYSQL_PASSWORD: admin
    volumes:                      // volumes which is attach with container to store data
      - ./mysql-data:/var/lib/mysql
      - ./message.sql:/docker-entrypoint-initdb.d/message.sql        // optioinal
    networks:
      - two-tier        // network name under which these container will run
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-uroot", "-proot"]      // healthcheck to ensure that , after successfully launching mysql cotainer , only then flas-app will become ready to connect 
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s

  flask-app:
    build:
      context: .
    container_name: flask-app
    ports:
      - "5000:5000"
    environment:
      MYSQL_HOST: mysql
      MYSQL_USER: root
      MYSQL_PASSWORD: root
      MYSQL_DB: devops
    depends_on:                  // when mysql container up , only then flask container will start making its image and upping its container 
      - mysql
    networks:
      - two-tier
    restart: always              // if during connection , it should be restart
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5000/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

networks:
  two-tier:
    driver: bridge


1. docker compose up / docker-compose up : this command used to run docker compose yml file , after using docker compose , we get free of writting docker run and docker build command again and again 

2. docker compose up -d : run in detach mode or in backgroud mode . 

3. docker rmi <imageId> : use to remove image 

4. docker images aq : it will give all images id . 

5. docker rmi -f $(docker images aq) : it will remove all docker images . 

6. docker compose down : it will remove all images . 

7. docker compose up -d --build : it will forcefully build image . 


================================================================= Multistage Docker Build =========================================================

A multistage Dockerfile is a feature introduced in Docker to address the challenge of creating lean and efficient container images Traditionally, Docker images used to
contain all the dependencies, libraries, and tools required to run an application, leading to bloated images that consume unnecessary disk space and hence increase the
deployment times Now Multistage builds allow developers to build multiple intermediate images within a single Dockerfile, and each intermediate image serves a specific
purpose in the build process.

In a multistage Dockerfile, developers define multiple build stages, each encapsulating a specific set of instructions and dependencies. These stages can be named and
referenced within the Dockerfile, enabling seamless communication between them. Basically, the first stage of creating a multistage Dockerfile is dedicated to building 
the application code, while subsequent stages focus on packaging the application and preparing it for runtime. Intermediate images that are generated in earlier stages 
are discarded just after their purpose is served, resulting in a final production image that contains only the essential components required to run the application.

# ------------------- Stage 1: Build Stage ------------------------------
FROM python:3.9 AS builder                 // big image to download all dependenciees 

WORKDIR /app

# Install build dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends gcc default-libmysqlclient-dev pkg-config && \
    rm -rf /var/lib/apt/lists/*

# Copy and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ------------------- Stage 2: Final Stage ------------------------------
FROM python:3.9-slim

WORKDIR /app

# Install runtime dependencies only
RUN apt-get update && \
    apt-get install -y --no-install-recommends libmariadb3 && \
    rm -rf /var/lib/apt/lists/*

# Copy dependencies and application code from the builder stage
COPY --from=builder /usr/local/lib/python3.9/site-packages/ /usr/local/lib/python3.9/site-packages/
COPY . .

CMD ["python", "app.py"]



================================================= LOgs and Monitoring ================================================================

  1. nohup docker attach <cotainerId> &  : this command will start storing application logs in output file nohup.out file

====================================================NGINX 3 Tier Application =======================================

Github : Shubham Londhe -> Repository -> django notes app

services:
  nginx:
    build:
      context: ./nginx
    container_name: "nginx_cont"
    ports:
      - "80:80"
    restart: always
    depends_on:
      - django
    networks:
      - notes-app

  django:
    build:
      context: .
    image: django
    container_name: "django_cont"
    ports:
      - "8000:8000"
    command: sh -c "python manage.py migrate --noinput && gunicorn notesapp.wsgi --bind 0.0.0.0:8000"
    env_file:
      - ".env"
    depends_on:
      - db
    restart: always
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/admin || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - notes-app

  db:
    image: mysql
    ports:
      - "3306:3306"
    container_name: "db_cont"
    environment:
      - MYSQL_ROOT_PASSWORD=root
      - MYSQL_DATABASE=test_db
    volumes:
      - ./mysql-data:/var/lib/mysql
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-uroot", "-proot"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s
    networks:
      - notes-app
    restart: always


volumes:
  mysql-data:

networks:
  notes-app:


----------------> Nginx default.conf file code 

upstream django{
    server django_cont:8000;
}

server {
    listen 80;

    server_name localhost;

    location / {
        proxy_pass http://django_cont:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}

-------------------------> Java Spring Boot APP docker compose file code -----------------

version: "3.8"
services:
  mysql:
    image: mysql:latest
    container_name: mysql
    environment:
      MYSQL_ROOT_PASSWORD: Test@123
      MYSQL_DATABASE: expenses_tracker
    volumes:
      - ./mysql-data:/var/lib/mysql
    networks:
      - appbridge
    healthcheck:
      test: ["CMD","mysqladmin","ping","-h","localhost","-uroot","-pTest@123"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: always

  mainapp:
    image: snehcreate/expensetracker_v3
    container_name: Expensetracker
    environment:
      SPRING_DATASOURCE_USERNAME: root
      SPRING_DATASOURCE_URL: "jdbc:mysql://mysql:3306/expenses_tracker?allowPublicKeyRetrieval=true&useSSL=false"
      SPRING_DATASOURCE_PASSWORD: Test@123
    ports:
      - "8080:8080"
    networks:
      - appbridge
    depends_on:
      - mysql
    restart: always

networks:
  appbridge:

volumes:
  mysql-data:


================================================Docker Scout =================================================
Docker scout is image scanning tool . It can scan vulnerabilities in image . 
Docker Scout is an innovative tool that simplifies securing Docker images by analyzing their contents and generating a 
detailed report of any vulnerabilities detected during the process. Docker Scout’s key features include inspecting for
common vulnerabilities and exposures (CVE), providing security recommendations, and seamless integration with continuous
integration, continuous delivery (CI/CD) workflows, helping you discover and remediate vulnerabilities during development.

    docker scout quickview <image name> : This command will give all vulnerabilities of image . One need to login with docker hub to run this command . 

    docker scout cves <image name> : This command used to go more deep in vulnerabilities . cves : common vulnerabilities end scan


    Docker INIT -> this will generate all docker related file 

