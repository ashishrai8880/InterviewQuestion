
Reference : https://medium.com/geekculture/system-design-design-a-rate-limiter-81d200c9d392

 ================================================= Rate Limiter Techniques ===============================================================

A Rate Limiter limits the number of client requests allowed to be sent over a specified period. If the API request count exceeds the threshold
defined by the rate limiter, all the excess calls are blocked.

A rate limiter prevents DoS attacks, intentional or unintentional, by blocking the excess calls.
Reduces cost where the system is using a 3rd-party API service and is charged on a per-call-basis.
To reduce server load, a rate limiter is used to filter out excess requests caused by bots or users‚Äô misbehaviour.


==================================== 1. Token Bucket Algorithm  ==========================================================================
Stripe (https://stripe.com/blog/rate-limiters) uses a token bucket algorithm to throttle their API requests.

We use the token bucket algorithm to do rate limiting. This algorithm has a centralized bucket host where you take tokens on
each request, and slowly drip more tokens into the bucket. If the bucket is empty, reject the request. In our case, every
Stripe user has a bucket, and every time they make a request we remove a token from that bucket. We implement our rate 
limiters using Redis.

The bucket has tokens with a pre-defined capacity. When a request comes in it takes a token from the bucket and is
processed further. If there is no token to be picked, the request is dropped and the user will have to retry.

We have our rate-limiting rules set to 3 requests per user per minute.
1. User1 makes the request at 00 time interval, currently the bucket is full with 3 tokens so request will be processed.
Tokens will be updated in the bucket to 2 now.
2. User1 makes a 2nd request at 10th sec, bucket has 2 tokens so again the request will be processed further.
Tokens will be updated in the bucket to 1 now.
3. User1 makes a 3rd request at say 30th sec, bucket has 1 token so again the request will be processed further.
Now the bucket is empty till the whole 1 minute completes.
4. User1 makes a 4th request at say 55th sec, the bucket has 0 tokens so the request is throttled and the user is returned 
with a 429 status code ‚Äî too many requests and asked to retry it later in some time. The HTTP 429 Too Many Requests response
status code indicates the user has sent too many requests in a given amount of time.
5. At the completion of that 1 minute, the token count is refreshed at a fixed rate and the bucket is again full with 3 tokens
and requests can be processed again for that particular user.

In simple words:
In the Token Bucket algorithm, we process a token from the bucket for every request. New tokens are added to the bucket with rate r.
The bucket can hold a maximum of b tokens. If a request comes and the bucket is full it is discarded.

=========== Implementation In Nodejs Interview perspective =========
In-memory token bucket (single server)

class TokenBucket {
  constructor(capacity, refillRate) {
    this.capacity = capacity;        // max tokens
    this.refillRate = refillRate;    // tokens per second
    this.tokens = capacity;          // current tokens
    this.lastRefill = Date.now();     // last refill time
  }

  allowRequest() {
    this.refillTokens();

    if (this.tokens >= 1) {
      this.tokens -= 1;
      return true;
    }

    return false;
  }

  refillTokens() {
    const now = Date.now();
    const elapsedSeconds = (now - this.lastRefill) / 1000;

    const tokensToAdd = elapsedSeconds * this.refillRate;
    this.tokens = Math.min(
      this.capacity,
      this.tokens + tokensToAdd
    );

    this.lastRefill = now;
  }
}

4Ô∏è‚É£ Express usage (optional in interview)
const bucket = new TokenBucket(10, 1);

app.use((req, res, next) => {
  if (!bucket.allowRequest()) {
    return res.status(429).send("Too Many Requests");
  }
  next();
});

5Ô∏è‚É£ Now the IMPORTANT INTERVIEW QUESTION üî•
‚ÄúWhat are the problems with this?‚Äù

You say:

‚ùå Works only on one server
‚ùå Memory resets on restart
‚ùå Multiple instances won‚Äôt share state

This is when interviewer asks:

‚ÄúHow would you scale this?‚Äù

6Ô∏è‚É£ Scaling idea (EXPLAIN, don‚Äôt code)

You say:
‚ÄúTo scale, I would store the token bucket state in a shared store like Redis so all servers see the same tokens.‚Äù

Then interviewer might ask:
‚ÄúBut how do you avoid race conditions?‚Äù

7Ô∏è‚É£ Enter Lua ‚Äî Explained VERY SIMPLY
What is Lua?
Lua is a small scripting language that Redis can run inside itself.
Why do we use it?

Because:
Redis normally does:
GET
calculate in Node.js
SET

That causes race conditions

Lua does:
READ ‚Üí CALCULATE ‚Üí WRITE
ALL IN ONE STEP
NO other request can interrupt it

üëâ This is called atomic execution.

9Ô∏è‚É£ How Redis + Lua version works (NO CODE)
Say this in interview:
‚ÄúEach user has a key in Redis.
The key stores:
number of tokens
last refill timestamp

When a request comes:
Redis script checks last refill time
Calculates how many tokens should be added

Updates token count
If token ‚â• 1 ‚Üí allow request
Else ‚Üí reject
This entire logic runs atomically inside Redis using Lua.‚Äù
That answer is üî•.

üîü One-liner explanation (INTERVIEW GOLD)
‚ÄúToken bucket allows controlled bursts while maintaining an average rate.
Tokens refill based on elapsed time, and using Redis + Lua makes the operation atomic and scalable across servers.‚Äù

=========================== Production level scalable token bucket rate limiter with lua script ==================================
-- token_bucket.lua
local key = KEYS[1]

local capacity       = tonumber(ARGV[1])
local refill_rate    = tonumber(ARGV[2]) -- tokens per second
local now            = tonumber(ARGV[3])
local requested      = tonumber(ARGV[4])

local data = redis.call("HMGET", key, "tokens", "last_refill")
local tokens = tonumber(data[1])
local last_refill = tonumber(data[2])

if tokens == nil then
  tokens = capacity
  last_refill = now
end

-- refill tokens
local delta = math.max(0, now - last_refill) / 1000
local refill = delta * refill_rate
tokens = math.min(capacity, tokens + refill)

local allowed = tokens >= requested
if allowed then
  tokens = tokens - requested
end

redis.call("HMSET", key, "tokens", tokens, "last_refill", now)
redis.call("EXPIRE", key, math.ceil(capacity / refill_rate * 2))

return { allowed and 1 or 0, tokens }


// redis.js connection file code
// ------ redis.js
import Redis from "ioredis";

export const redis = new Redis({
  host: "127.0.0.1",
  port: 6379,
  maxRetriesPerRequest: 3,
});


// ------- rateLimiter.js 
// rateLimiter.js
import fs from "fs";
import path from "path";
import { redis } from "./redis.js";

const luaScript = fs.readFileSync(
  path.resolve("token_bucket.lua"),
  "utf8"
);

const SCRIPT_SHA = await redis.script("LOAD", luaScript);

export class TokenBucketLimiter {
  constructor({
    capacity,
    refillRate,
    keyPrefix = "rate:",
  }) {
    this.capacity = capacity;
    this.refillRate = refillRate;
    this.keyPrefix = keyPrefix;
  }

  async allow(key, tokens = 1) {
    const redisKey = this.keyPrefix + key;
    const now = Date.now();

    const [allowed, remaining] = await redis.evalsha(
      SCRIPT_SHA,
      1,
      redisKey,
      this.capacity,
      this.refillRate,
      now,
      tokens
    );

    return {
      allowed: allowed === 1,
      remaining,
    };
  }
}


// express middleware 
// middleware.js
import { TokenBucketLimiter } from "./rateLimiter.js";

const limiter = new TokenBucketLimiter({
  capacity: 100,       // burst
  refillRate: 10,      // tokens/sec
});

export const rateLimit = (keyFn) => async (req, res, next) => {
  try {
    const key = keyFn(req);
    const result = await limiter.allow(key);

    if (!result.allowed) {
      return res.status(429).json({
        error: "Too Many Requests",
      });
    }

    res.setHeader("X-RateLimit-Remaining", Math.floor(result.remaining));
    next();
  } catch (err) {
    next(err);
  }
};


// -------------- inside app.js file =====================
// app.js
import express from "express";
import { rateLimit } from "./middleware.js";

const app = express();

app.use(
  "/api",
  rateLimit(req => req.ip) // or userId / apiKey
);

app.get("/api/data", (req, res) => {
  res.json({ success: true });
});

app.listen(3000);


===================================================== 2. Leaky Bucket Algorithm ======================================================

Leaky Bucket is a simple and intuitive way to implement rate limiting using a queue. It is a simple first-in, first-out queue (FIFO).
Incoming requests are appended to the queue and if there is no room for new requests they are discarded (leaked).

When a request arrives, the system checks if the queue is full. If it is not full, the request is added to the queue.
Otherwise, the request is dropped.
Requests are pulled from the queue and processed at regular intervals.

The leaking bucket algorithm takes the following two parameters:

Bucket size: it is equal to the queue size. The queue holds the requests to be processed at a fixed rate.
Outflow rate: it defines how many requests can be processed at a fixed rate, usually in seconds.
This algorithm‚Äôs advantage is that it smooths out bursts of requests and processes them at an approximately average rate.

// simple implementation
class LeakyBucket {
  constructor(capacity, leakRatePerSecond) {
    this.capacity = capacity;
    this.leakRate = leakRatePerSecond;
    this.queueSize = 0;

    // Leak at fixed interval
    setInterval(() => {
      if (this.queueSize > 0) {
        this.queueSize--;
      }
    }, 1000 / this.leakRate);
  }

  allowRequest() {
    if (this.queueSize >= this.capacity) {
      return false; // bucket overflow
    }
    this.queueSize++;
    return true;
  }
}


// using in middleware 
const buckets = new Map();

function rateLimiter(req, res, next) {
  const userId = req.ip;

  if (!buckets.has(userId)) {
    buckets.set(userId, new LeakyBucket(10, 2)); 
    // capacity = 10, leak rate = 2 req/sec
  }

  const bucket = buckets.get(userId);

  if (!bucket.allowRequest()) {
    return res.status(429).json({ message: "Too Many Requests" });
  }

  next();
}

üîπ How to Explain This in Interview

‚ÄúEach user has a bucket with a fixed capacity.
Incoming requests increase the bucket size.
A background process leaks requests at a constant rate.
If the bucket overflows, requests are rejected.‚Äù

3Ô∏è‚É£ Problems With This Simple Design (Say This!)
Interviewers expect you to critique your own solution.

‚ùå Limitations
Not scalable (single instance)
Data lost on restart
Memory grows with users
Multiple servers = inconsistent limits

Say:
‚ÄúThis works well for a single-node system, but breaks in distributed environments.‚Äù


4Ô∏è‚É£ Improved Design (Still Simple, No Queue)

Instead of an actual queue, we simulate leaking using timestamps.
Key Insight
We track:
waterLevel
lastLeakTimestamp
Then calculate how much leaked on demand.

üîπ Improved Leaky Bucket Logic

class LeakyBucket {
  constructor(capacity, leakRatePerSec) {
    this.capacity = capacity;
    this.leakRate = leakRatePerSec;
    this.water = 0;
    this.lastLeak = Date.now();
  }

  allowRequest() {
    const now = Date.now();
    const elapsed = (now - this.lastLeak) / 1000;

    // Leak water
    this.water = Math.max(0, this.water - elapsed * this.leakRate);
    this.lastLeak = now;

    if (this.water + 1 > this.capacity) {
      return false;
    }

    this.water++;
    return true;
  }
}


Why this is better

‚úî No timers
‚úî O(1) per request
‚úî Easy to store in Redis later

5Ô∏è‚É£ Scalable Distributed Leaky Bucket (Interview Gold ‚≠ê)
Now we upgrade to system design level.
  üîπ Architecture
  Client
  ‚Üì
  Load Balancer
    ‚Üì
  API Servers
    ‚Üì
  Redis (Shared State)

6Ô∏è‚É£ Redis-Based Leaky Bucket (Production Style)
Data Stored per User
user:123 ‚Üí {
  water: 4.5
  lastLeak: 1690000000
}

üîπ Redis Pseudo Code

function allowRequest(userId) {
  const now = currentTime();
  const bucket = redis.get(userId);

  const elapsed = now - bucket.lastLeak;
  const leaked = elapsed * LEAK_RATE;

  const water = Math.max(0, bucket.water - leaked);

  if (water + 1 > CAPACITY) {
    return false;
  }

  redis.set(userId, {
    water: water + 1,
    lastLeak: now
  });

  return true;
}

üîπ Atomicity (Important Interview Point)

Use:
Redis Lua script or MULTI / WATCH

Say:
‚ÄúTo prevent race conditions across servers, I use Redis Lua scripts for atomic updates.‚Äù

7Ô∏è‚É£ Lua Script (Advanced Interview Bonus)
local capacity = tonumber(ARGV[1])
local leakRate = tonumber(ARGV[2])
local now = tonumber(ARGV[3])

local water = tonumber(redis.call("HGET", KEYS[1], "water") or "0")
local lastLeak = tonumber(redis.call("HGET", KEYS[1], "lastLeak") or now)

local leaked = (now - lastLeak) * leakRate
water = math.max(0, water - leaked)

if water + 1 > capacity then
  return 0
end

redis.call("HSET", KEYS[1], "water", water + 1)
redis.call("HSET", KEYS[1], "lastLeak", now)
redis.call("EXPIRE", KEYS[1], 60)

return 1

8Ô∏è‚É£ How to Explain Scalability in Interview

Use this flow:
Start with in-memory
Identify limitations
Move state to Redis
Ensure atomic updates
Use TTL for cleanup

One-liner:
‚ÄúBy moving the bucket state to Redis and using atomic Lua scripts, the rate limiter becomes horizontally scalable.‚Äù


==================================================== Sliding Window Algorithm ===================================================
Sliding Window Algorithm
The algorithm keeps track of request timestamps. Timestamp data is usually kept in cache, such as sorted sets of Redis.
When a new request comes in, remove all the outdated timestamps. Outdated timestamps are defined as those older than the start of the current time window.
Add timestamp of the new request to the log.
If the log size is the same or lower than the allowed count, a request is accepted. Otherwise, it is rejected.





// ==================================== PROBLEM IN RATE LIMITERS ====================================================
1. Race condition

As discussed above:

Read the counter value from Redis and check if ( counter + 1 ) exceeds the threshold. If not, increment the counter value by 1 in
Redis.

Press enter or click to view image in full size

Image source System Design Interview ‚Äî An insider‚Äôs guide by Alex Xu
Assume the counter value in Redis is 3 (as shown in the image above). If two requests concurrently read the counter value before
either of them writes the value back, each will increment the counter by one and write it back without checking the other thread.
Both requests (threads) believe they have the correct counter value 4. However, the correct counter value should be 5.

Locks can be useful here, but that might impact the performance. So we can use Redis Lua Scripts.

This may result in better performance.
Also, all steps within a script are executed in an atomic way. No other Redis command can run while a script is executing.

2. Synchronization

Synchronization is another important factor to consider in a distributed environment. To support millions of users, one rate 
limiter server might not be enough to handle the traffic. When multiple rate limiter servers are used, synchronization is required.

One possible solution is to use sticky sessions that allow a client to send traffic to the same rate limiter. This solution is not 
advisable because it is neither scalable nor flexible. A better approach is to use centralized data stores like Redis.

After things are in place, we can also monitor our rate limiter for metrics like performance, rules, algorithm effectiveness, etc. 
For example, if rate-limiting rules are too strict, many valid requests are dropped. In this case, we want to relax the rules a
little bit. In another example, we notice our rate limiter becomes ineffective when there is a sudden increase in traffic like 
flash sales. In this scenario, we may replace the algorithm to support burst traffic. The token bucket is a good fit here.











