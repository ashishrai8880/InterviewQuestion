
1. Caching

    Locality of reference principle  
    Best Article : https://medium.com/@shinchan76/locality-of-reference-and-its-type-cache-memory-74baa6644eae

    * Locality of Reference means programs often reuse the same data or nearby instructions rather than accessing random memory locations.
    * Locality of reference is the property of a program that describes how memory references are grouped together in time and space during execution.

    Two Types of locality of referecne 

      1. Temporal Locality : Temporal locality states that if a particular memory location (data or instruction) is accessed at one time, it is likely to be accessed again in the near future.
      2. Spatial Locality : Spatial locality states that if a particular memory location is accessed, the nearby memory locations are also likely to be accessed soon. Instructions and data are
            stored in contiguous memory locations.


      Client   <---------->  Web Server <----------->  Application Server  <-----------> Database


    *********************************************************************************************************************
 ************************************************************Cache Eviction Policy ********************************

Medium Link : https://medium.com/@premchandu.in/7-cache-eviction-strategies-you-should-know-0bc4f08fd414

It basically means , which should be put in cache and which should be removed . Because we can have limited cache space . 
There are multiple ways of it . 

1. Least Recently Used (LRU) - LRU evicts the item that hasn‚Äôt been used for the longest time.
The idea is simple: if you haven‚Äôt accessed an item in a while, it‚Äôs less likely to be accessed again soon.

    How it Works
        - Access Tracking: LRU keeps track of when each item in the cache was last accessed. This can be done using various data                 structures, such as a doubly linked list or a combination of a hash map and a queue.
        - Cache Hit (Item Found in Cache): When an item is accessed, it is moved to the most recently used position in the tracking               data structure (e.g., moving it to the front of a list).
        - Cache Miss (Item Not Found in Cache):
        - If the item isn‚Äôt in the cache and the cache has free space, it is added directly.
        - If the cache is full, the least recently used item is evicted to make space for the new item.
        - Eviction: The item that has been accessed least recently (tracked at the beginning of the list) is removed from the cache.


 ## Implementatioin

    
class LRUCache {
  constructor(capacity) {
    this.capacity = capacity;
    this.cache = new Map();
  }

  get(key) {
    if (!this.cache.has(key)) {
      return -1;
    }

    // Move key to the end to mark it as recently used
    const value = this.cache.get(key);
    this.cache.delete(key);
    this.cache.set(key, value);

    return value;
  }

  put(key, value) {
    // If key exists, remove it first
    if (this.cache.has(key)) {
      this.cache.delete(key);
    }

    // If capacity exceeded, remove least recently used item
    if (this.cache.size >= this.capacity) {
      const lruKey = this.cache.keys().next().value;
      this.cache.delete(lruKey);
    }

    // Insert the new key-value pair
    this.cache.set(key, value);
  }
}


const lru = new LRUCache(2)

lru.put(1, 1);
lru.put(2, 2);

console.log(lru.get(1)); // 1

lru.put(3, 3); // Evicts key 2

console.log(lru.get(2)); // -1 (not found)

lru.put(4, 4); // Evicts key 1


## Explantion : next() method is magical method . One more thing , map maintains the order inside it unlike other programing 
language . so logic is , when insertion in map is greater than capacity , then simply it takes out first insertion and delete it . 


******** 2. LFU Least Frequently Used : LFU evicts the item with the lowest access frequency. It assumes that items accessed less 
frequently in the past are less likely to be accessed in the future.
Unlike LRU, which focuses on recency, LFU emphasizes frequency of access.

## Implementation

class LFUCache {
  constructor(capacity) {
    this.capacity = capacity;
    this.size = 0;

    this.minFreq = 0;
    this.keyToValFreq = new Map(); // key -> { value, freq }
    this.freqToKeys = new Map();   // freq -> Map of keys
  }

  get(key) {
    if (!this.keyToValFreq.has(key)) {
      return -1;
    }

    const entry = this.keyToValFreq.get(key);
    this._updateFreq(key, entry);

    return entry.value;
  }

  put(key, value) {
    if (this.capacity === 0) return;

    if (this.keyToValFreq.has(key)) {
      const entry = this.keyToValFreq.get(key);
      entry.value = value;
      this._updateFreq(key, entry);
      return;
    }

    if (this.size >= this.capacity) {
      this._evictLFU();
    }

    // Insert new key
    this.keyToValFreq.set(key, { value, freq: 1 });

    if (!this.freqToKeys.has(1)) {
      this.freqToKeys.set(1, new Map());
    }

    this.freqToKeys.get(1).set(key, true);
    this.minFreq = 1;
    this.size++;
  }

  _updateFreq(key, entry) {
    const freq = entry.freq;

    // Remove from old frequency list
    const keys = this.freqToKeys.get(freq);
    keys.delete(key);

    if (keys.size === 0) {
      this.freqToKeys.delete(freq);
      if (this.minFreq === freq) {
        this.minFreq++;
      }
    }

    // Add to new frequency list
    entry.freq++;
    const newFreq = entry.freq;

    if (!this.freqToKeys.has(newFreq)) {
      this.freqToKeys.set(newFreq, new Map());
    }

    this.freqToKeys.get(newFreq).set(key, true);
  }

  _evictLFU() {
    const keys = this.freqToKeys.get(this.minFreq);
    const lfuKey = keys.keys().next().value;

    keys.delete(lfuKey);
    if (keys.size === 0) {
      this.freqToKeys.delete(this.minFreq);
    }

    this.keyToValFreq.delete(lfuKey);
    this.size--;
  }
}


********* 3. FIFO (First In First Out ) : FIFO evicts the item that was added first, regardless of how often it‚Äôs accessed.
FIFO operates under the assumption that items added earliest are least likely to be needed as the cache fills up.

****** 4. Random Replacement (RR) : RR cache eviction strategy is the simplest of all: when the cache is full, it evicts a random
item to make space for a new one. 
It doesn‚Äôt track recency, frequency, or insertion order, making it a lightweight approach with minimal computational overhead.

****** 5. Most Recently Used (MRU)  : MRU is the opposite of Least Recently Used (LRU). In MRU, the item that was accessed most 
recently is the first to be evicted when the cache is full.

The idea behind MRU is that the most recently accessed item is likely to be a temporary need and won‚Äôt be accessed again soon, so 
evicting it frees up space for potentially more valuable data.

******** 6. Time to Live (TTL) : TTL is a cache eviction strategy where each cached item is assigned a fixed lifespan. Once an item‚Äôs
lifespan expires, it is 
automatically removed from the cache, regardless of access patterns or frequency.

******** 7. Two-Tiered Caching
Two-Tiered Caching combines two layers of cache ‚Äî usually a local cache (in-memory) and a remote cache (distributed or shared).

The local cache serves as the first layer (hot cache), providing ultra-fast access to frequently used data, while the remote cache 
acts as the second layer (cold cache) for items not found in the local cache but still needed relatively quickly.

********** REDIS has all its cache eviction policy inbuilt . No need to develope custom logic **********************

**** Cache Invalidation : Cache invalidation is a state where we push away the data from the cache memory. When the data present in 
cache is outdated. We perform this operation of pushing back or flushing the data from the cache. Otherwise, it will cause data 
inconsistency.


*********************************************************************************************************************************
**************************************************** Cache Update Strategies ****************************************************

Medium : https://medium.com/@mmoshikoo/cache-strategies-996e91c80303

1. Write Through Cache : Data is written to cache and database at the same time . Write operation is synchronous .
Client ‚Üí Cache ‚Üí Database

Key Characteristics
    Cache always has latest data
    Slower writes (because DB write is required)
    Very safe and simple

Pros
    ‚úÖ Strong consistency
    ‚úÖ Easy to implement
    ‚úÖ Cache never stale

Cons
    ‚ùå Higher write latency
    ‚ùå Database load increases

Common Use Cases
    Banking systems
    Financial transactions
    Systems requiring strong consistency .

2. Write Back Cache / Write Behind / Lazy Write (db me baad me write hoga) :  Data is written only to cache . Database update happens later (asynchronously)

    Client ‚Üí Cache ‚Üí (Later) Database

Key Characteristics
    Cache marked as dirty
    DB updated after delay or eviction

Pros
    ‚úÖ Very fast writes
    ‚úÖ Reduced DB load

Cons
    ‚ùå Risk of data loss if cache crashes
    ‚ùå More complex (dirty tracking, flush logic)

Common Use Cases
    High-write systems
    Analytics
    Event processing

3. Write-Around Cache / Cache Aside / Read Around : Data is written directly to DB . Cache is skipped on write . Cache updated only on read . When data is requested, the application checks the cache first. If the data is not in the cache, it is retrieved from the 
database and stored in the cache for future use. 

    Client ‚Üí Database
    Cache ‚Üê (On Read)

Key Characteristics
    Cache may not have newly written data
    Avoids cache pollution

Pros
    ‚úÖ Prevents useless cache entries
    ‚úÖ Good for write-heavy workloads

Cons
    ‚ùå First read is always a cache miss
    ‚ùå Slightly stale cache possible

Common Use Cases
Data written once, read rarely
Large objects
Logging systems

4. Write-Invalidate : On write, cache entry is invalidated . Fresh data loaded on next read

Key Characteristics
    Common in distributed caches
    Ensures readers fetch latest data

Pros
    ‚úÖ Avoids stale data
    ‚úÖ Simple consistency model

Cons
    ‚ùå Cache miss on next read
    ‚ùå Extra DB reads

Common Use Cases
    Distributed systems
    Multi-node caches

***********************************************************************************************************************************
1. Distributed Caching : Distributed caching is a technique where cache data is spread across multiple servers within a network,
enhancing performance and scalability by reducing load on primary data stores. It improves application responsiveness by storing
frequently accessed data closer to the application, reducing latency and improving overall system efficiency.

2. Centralised Caching : A centralised cache is a caching approach where one shared cache system stores frequently used data and is 
accessed by multiple applications, services, or servers.

Instead of each server keeping its own cache, everyone reads from and writes to the same central cache.

How it works (simple view)
    Data is stored in a single cache location
    Multiple clients or servers query that cache
    If the data is found (a cache hit), it‚Äôs returned quickly
    If not (a cache miss), the data is fetched from the main data source and then stored in the cache

Why use a centralised cache?

Advantages
    ‚úÖ Consistent data across all services
    ‚úÖ Reduced load on databases
    ‚úÖ Easier cache management
    ‚úÖ Better memory utilization than per-server caches

Disadvantages
    ‚ùå Single point of failure (unless replicated)
    ‚ùå Network latency (cache is accessed over the network)
    ‚ùå Scalability limits if not designed properly


***** Cache Coherency *********

Modern CPUs are multi-core.
Each core usually has its own cache (L1, sometimes L2), and all cores share the same main memory (RAM).

The problem
If multiple cores cache the same memory location, they might end up with different values.

Example:
    Core 1 reads x = 10 ‚Üí stores it in its cache
    Core 2 also reads x = 10 ‚Üí stores it in its cache
    Core 1 updates x = 20 (only its cache changes)
    Core 2 still thinks x = 10

üëâ Now the system is inconsistent.
    Cache coherency ensures that all cores see a consistent view of memory.

2. What cache coherency guarantees
A cache-coherent system ensures:
Writes by one core become visible to others
No core uses stale (old) data
Memory behaves as if there is a single, shared value for each address

Important note:
Cache coherency ‚â† memory consistency
Coherency is about one memory location; consistency is about ordering of multiple memory operations.


****************************** Difference between Redis and Memecache **********************************

One Major difference is , redis offer persistent storage while memecache does not offer . Most of the companies started using 
redis as a database . 

*************************************************************************************************************
********************************* PROXY ******************************

A proxy server acts as a middle layer between your device and the internet, hiding your real IP address and improving online privacy.
It routes your requests through another server, offering security, anonymity, and control.

Instead of a client (like your browser) talking directly to a website or backend server, it talks to a proxy, which then forwards the
request on its behalf.

Client ‚Üí Proxy ‚Üí Server
Server ‚Üí Proxy ‚Üí Client

Why Proxies Are Used
    Common reasons include:
    Security (hide internal systems)
    Privacy (hide client identity)
    Performance (caching)
    Control (filtering, logging, rate-limiting)
    Load balancing

1. Forward Proxy (Client-side Proxy) 
A forward proxy sits in front of the client and represents the client.
It is typically used when clients want controlled access to the internet.

Key Idea
üëâ The server does NOT know the real client
üëâ The client knows the proxy

Client ‚Üí Forward Proxy ‚Üí Internet Server
Example:
Your Browser ‚Üí Corporate Proxy ‚Üí google.com

Real-Life Example
Company network where:
    Employees access the internet
    All traffic goes through a proxy
    Certain websites are blocked
    Activity is logged

Common Use Cases
    Internet access control (schools, offices)
    Content filtering (block social media)
    Anonymity (hide IP address)
    Caching frequently accessed websites

Client requests https://example.com
‚Üì
Forward Proxy sends request on behalf of client
‚Üì
example.com responds to proxy
‚Üì
Proxy sends response back to client

2. Reverse Proxy (Server-side Proxy)
A reverse proxy sits in front of one or more servers and represents the server(s).
Clients think they are talking to the real server, but they‚Äôre actually talking to the reverse proxy.

Key Idea
üëâ The client does NOT know about backend servers
üëâ The server knows the proxy

Client ‚Üí Reverse Proxy ‚Üí Backend Server(s)
Example:
Browser ‚Üí NGINX ‚Üí App Server

Real-Life Example
    A website like amazon.com
    Users connect to Amazon
    Traffic first hits a reverse proxy
    Proxy distributes traffic to many backend services

Common Use Cases
    Load balancing
    SSL termination (HTTPS handling)
    Security (hide backend servers)
    Caching
    Rate limiting    
    API gateway functionality

Client requests https://myapp.com
‚Üì
Reverse Proxy receives request
‚Üì
Chooses a backend server
‚Üì
Forwards request
‚Üì
Returns response to client


***********************************************************************************************************************
************************************** Load Balancing Strategies *******************************************

Best Link : https://vertisystem.medium.com/10-load-balancing-techniques-mastering-the-art-of-distributed-computing-9ab053ad138f

1. Round Robin Strategies
Round Robin is one of the simplest algorithms used in load balancing. In this method, incoming requests are distributed in a circular
order across all the available servers in a server pool
Pros : 
    Simplicity , Fairness , Low Overhead , Predictablity , Quick Response 
Cons : 
    Unaware of server load .
    Not session aware :  In a stateful application where user sessions are important, Round Robin can break the application logic 
        because it does not ensure session persistence. For example, if a user logs in on Server A and the next request goes to 
        Server B, the user might be logged out if sessions are not shared between the servers.
    Inefficient Resource Utilization: If servers have different capabilities (e.g., different CPU, RAM), Round Robin will not exploit
        these differences efficiently.


2. Least Connection Strategies
The Least Connections algorithm is a more advanced load balancing technique compared to Round Robin. In this method, the load 
balancer maintains a record of the number of active connections for each server in the pool. Incoming requests are routed to the
server with the least number of active connections at that moment.

Pros : More Intelligent routing , better for varying request complexity , good for long-lived connections , improves over time . 
Cons : 
    Unaware of task complexity  : While the Least Connections method considers the number of active connections, it doesn‚Äôt account 
for the computational complexity of the tasks each server is handling. A server could have fewer but more resource-intensive tasks,
and yet still receive new requests.
    Higher Overhead : Maintaining a real-time count of active connections for each server incurs more computational overhead compared
to simpler algorithms like Round Robin.
    Potential for Suboptimal Utilization: If servers are of varying capacities, the least capable server may receive the same number 
of requests as the most capable server, which could result in suboptimal resource utilization.
    Initial Imbalance: In some setups, when all servers have zero or the same number of connections, the first few requests could all go to the same server, creating a temporary imbalance.


3. Weighted Round Robin and  Weighted Least Connection : Both Weighted Round Robin and Weighted Least Connections are extensions of
the basic Round Robin and Least Connections algorithms, respectively. In these weighted methods, each server is assigned a weight 
based on its capacity or some other criteria. Servers with higher weights will receive a proportionally larger number of requests.
Pros : Better resource utilization , adaptability , fairness and priority , scalability , hybrid scenarios .
Cons : 
   -  Complexity: The added layer of weights increases the complexity of managing and configuring the load balancer.
   -  Manual Tuning: Deciding the appropriate weights for each server often requires manual tuning and a deep understanding of the 
workload characteristics and server capabilities. 
    - Resource Monitoring: For Weighted Least Connections, the load balancer has to not only keep track of active connections but also needs to consider the weights in real-time, increasing the computational overhead.
   -  Inefficiency in Weight Allocation: If weights are not set properly, it can lead to inefficient use of resources. For example, assigning a too-high weight to a low-capacity server can overwhelm it.

4. Hashing : Hashing is a technique used to map data to a fixed-size array, typically called a hash table. A hash function processes 
the input data to produce an output called a hash code, which is then used to index into the hash table. Hashing is commonly used for
efficient data retrieval, data indexing, data deduplication, and various other applications.
    Pros : 
        1. Speed vs Space : A larger hash table reduces collisions but uses more memory. Conversely, a smaller table uses less memory
but is more prone to collisions, which would then require additional time for resolution.
        2. Simple Vs Complex Function :  Simple hash functions are generally faster but may produce more collisions. Complex or 
cryptographic functions reduce the risk of collision or reverse engineering but are computationally more expensive.

    Cons : 
        Collisions , Non invertible , limited security , space time tradeoff , non ordered

5. Random Ordered : Random Allocation is a load-balancing strategy where incoming requests are distributed randomly across available
servers. Unlike algorithms like Round Robin or Least Connections, Random Allocation doesn‚Äôt follow a predefined pattern or consider 
any server metrics; it simply picks a server at random each time a request comes in.

    Pros : No need for tracking , uniformity over time , simplicity vs effficiency 
    Cons : Unpredictable load , no considereation for server capacity , not ideal for sticky session . 

6.. Geographical Based Connection : Geo-Location Based Load Balancing is a technique used to distribute incoming network or 
application traffic based on the geographic locations of the client and the server. This method helps in minimizing latency, as a 
user‚Äôs requests are generally routed to the closest or most appropriate data center.

Cons : Geo Ip Inaccuracy , Handling mobile users , regulatory and data sovereignty issues , complexity 
Pros : Cost vs performance , local optimization 

7. Application Layer Content Switching
Application Layer Content Switching is a sophisticated form of load balancing that distributes incoming requests based on content type, application state, or other application-layer data. This method is highly flexible and can route traffic based on a variety of factors such as URL paths, cookies, HTTP headers, or even custom application logic.


8. Resource-Based Load Balancing
Resource-Based Load Balancing is a strategy that considers the resources (CPU load, memory usage, network I/O, etc.) available on each server before distributing incoming requests. This approach aims to ensure that each server‚Äôs workload corresponds to its resource availability, leading to better performance and utilization.


******************************************************************************************************************************
*************************************************** CDN ****************************************************************

A CDN is a network of servers distributed around the world that deliver your website‚Äôs content from the location closest to the user.

Why should you use a CDN?
Main benefits
‚úÖ Faster page load times
‚úÖ Better performance for global users
‚úÖ Reduced load on your main server
‚úÖ Better reliability (traffic spikes, DDoS protection)
‚úÖ Often improves SEO and user experience

Method 1: Use a CDN provider (Easiest & Most common)
Popular CDN providers:
Cloudflare (very beginner-friendly)
Fastly
Akamai
Bunny.net
AWS CloudFront












