
------------------------ Short Polling , Long Polling , Websocket , SSE ---------------------------------------------------

1. ******* Short Polling 

Short polling is a client–server communication technique.
The client periodically sends HTTP requests to the server at fixed intervals to check for updates.
The server responds immediately, even if there is no new data.

Example: Client hits /api/notifications every 5 seconds.

=> How Short Polling Works
Client sends a request to the server.
Server processes the request and returns the current data.
Client waits for a fixed interval.
Client sends another request.
Cycle repeats.

=> When to Use Short Polling
Real-time updates not critical
Simple applications
Low number of users
Legacy systems where WebSockets aren’t available

=> Pros of Short Polling
✅ Simple to implement
✅ Works with standard HTTP
✅ Supported by all browsers
✅ Easy to debug
✅ No persistent connection required

Cons of Short Polling
❌ High server load (many repeated requests)
❌ Wasted bandwidth (requests even when no updates)
❌ Higher latency (data only fetched at intervals)
❌ Not suitable for real-time applications
❌ Poor scalability with many clients

====> Implementation
function pollData() {
  fetch('/api/data')
    .then(response => response.json())
    .then(data => {
      console.log('Received:', data);
    })
    .catch(error => {
      console.error('Error:', error);
    });
}

// Poll every 5 seconds
setInterval(pollData, 5000);



************************************** Long Polling ********************************************
Long polling is a client–server communication technique.
The client sends a request and the server holds the request open until:
    New data is available OR
    A timeout occurs.
Once the response is received, the client immediately sends a new request.
Used to simulate real-time behavior over HTTP.


==> How Long Polling Works

Client sends a request to the server.
Server does not respond immediately.
Server waits until:
    New data is available, or
    Request times out.
Server sends the response.
Client processes data.
Client immediately sends another request.
Cycle repeats.

==>When to Use Long Polling

Near real-time updates needed
WebSockets not available
Moderate number of users
Chat apps, notifications, live dashboards

Pros of Long Polling

✅ Lower latency than short polling
✅ Reduces unnecessary requests
✅ Works over standard HTTP
✅ No persistent connection like WebSockets
✅ Better server efficiency than short polling

Cons of Long Polling

❌ More complex than short polling
❌ Server holds connections open (resource usage)
❌ Scalability issues with many clients
❌ Timeout handling required
❌ Still not true real-time

=======> Implementation 
function longPoll() {
  fetch('/api/updates')
    .then(response => response.json())
    .then(data => {
      console.log('Received:', data);
      // Immediately start next long poll
      longPoll();
    })
    .catch(error => {
      console.error('Error:', error);
      // Retry after delay in case of error
      setTimeout(longPoll, 3000);
    });
}

// Start long polling
longPoll();


****************************************** Websocket **********************************************

WebSocket is a protocol that provides full-duplex, bidirectional communication between client and server.
Uses a single persistent TCP connection.
Designed for real-time applications.
Unlike polling, the server can push data to the client anytime.

How WebSockets Work
Client sends an HTTP handshake request.
Server accepts and upgrades the connection to WebSocket.
Persistent connection is established.
Client and server exchange data in real time.
Connection stays open until explicitly closed.

===> When to Use WebSockets
Real-time communication required
Chat applications
Live notifications
Multiplayer games
Live trading / stock updates
Collaborative apps (Google Docs–like)

===> Pros of WebSockets
✅ True real-time communication
✅ Low latency
✅ Full-duplex (both sides send anytime)
✅ Efficient (no repeated HTTP headers)
✅ Scales well for real-time use cases

Cons of WebSockets
❌ More complex to implement
❌ Requires stateful connections
❌ Harder to scale horizontally
❌ Needs connection management (reconnect, heartbeat)
❌ Not ideal for simple request-response APIs .

*** Imp : When client sends request to server in http format . header contains 2 things 
    1. upgrade : 'websocket'
    2. connection : upgrade 

    From this header , server knows to switch protocols from http to websocket . Switch protocol status code 101 . 
    Under the hood , socket also uses TCP protocol . 


---------------------------------- Server-Sent Events (SSE) ------------------------------------------------

Server-Sent Events (SSE) is a one-way communication mechanism.
Server can push updates to the client over a persistent HTTP connection.
Communication is server → client only.
Built on top of standard HTTP.
Ideal when the server needs to continuously send updates.

==> How SSE Works
Client opens a connection using EventSource.
Server keeps the HTTP connection open.
Server sends data whenever events occur.
Client receives events automatically.
Connection stays open until closed or interrupted.
Browser auto-reconnects if connection drops.

===> When to Use SSE
Live notifications
News feeds
Stock prices
Live dashboards
Logs / monitoring updates
Real-time analytics

====> Pros of SSE
✅ Simple to implement
✅ Uses standard HTTP (works with proxies & firewalls)
✅ Automatic reconnection handled by browser
✅ Efficient for server → client updates
✅ Less overhead than polling

====> Cons of SSE
❌ One-way communication only
❌ Not supported in older browsers (e.g., IE)
❌ Limited to text data (UTF-8)
❌ Not ideal for high-frequency bi-directional use cases
❌ Requires separate API for client → server communication

********** Implementation ***************
client side 

const eventSource = new EventSource('/events');

eventSource.onmessage = (event) => {
  console.log('Received:', event.data);
};

eventSource.onerror = () => {
  console.error('Connection error');
};

server side
data: New message arrived

data: Another update

** Uses text/event-stream content type
** Each message is separated by a blank line


------------------------------------------ Microservices ---------------------------------------------

1. Strangler Fig Pattern 
The Strangler Fig Pattern is a safe way to convert a monolithic application into microservices gradually, instead of 
rewriting everything at once.

It’s named after the strangler fig tree, which grows around a big tree, slowly replaces it, and eventually becomes the main
tree.

==> How It Works (Step by Step)

Start with a monolith
    One big application handling everything

Add a routing layer (API Gateway / Proxy)
    This decides where requests should go

Create a new microservice
    Example: User Service, Order Service

Route specific requests to the new service
    Old features → monolith
    New or migrated features → microservice

Repeat
    Gradually “strangle” the monolith
    Until it’s fully replaced

Very Simple Example

Monolith has:
User Management
Payments
Orders

Migration using Strangler Pattern:
    Create a Payment microservice
    Route /payments requests to it
    Monolith still handles Users & Orders
    Later move Users → Orders
    Finally remove the monolith

Why Use Strangler Fig Pattern?
✅ Advantages
No big-bang rewrite
Lower risk
Continuous delivery
Easy rollback if something fails
Works well for legacy systems

❌ Disadvantages
Temporary complexity
Requires good routing & monitoring
Needs strong discipline and planning

===> When Should You Use It?
Large legacy monolith
Business cannot afford downtime
Gradual migration preferred
Microservices adoption over time


*************************************************************************************************************************
************************************ RPC Remote Procedure Call gRPC *****************************************************

RPC (Remote Procedure Call)
What is RPC?
RPC allows a program to call a function on another machine as if it were a local function call.

Goal: Hide network communication and make distributed calls feel local.

 - How RPC Works (High Level)

1.Client calls a local stub
2. Stub serializes request (marshalling)
3. Request sent over network
4. Server deserializes request
5. Server executes function
6. Response serialized and sent back
7. Client receives response

Key Components

1. Client Stub – acts like a local function
2. Server Stub (Skeleton) – receives and invokes real function
3. IDL (Interface Definition Language) – defines function signatures
4. Transport – TCP / UDP / HTTP
5. Serialization – JSON / XML / Binary

Types of RPC

1. Synchronous RPC – client blocks until response
2. Asynchronous RPC – client continues execution
3. One-way RPC – no response expected

Advantages

1. Easy to understand (function calls)
2. Abstracts network complexity
3. Language-agnostic (with IDL)

Disadvantages

1. Network failures are hidden → false sense of reliability
2. Tight coupling between client & server
3. Harder to debug than REST
4. Versioning issues

Popular RPC Frameworks

1. gRPC
2. Apache Thrift
3. JSON-RPC
4. XML-RPC
5. CORBA


---gRPC (Google Remote Procedure Call)
What is gRPC?

gRPC is a modern, high-performance RPC framework developed by Google.

Used for microservices, low-latency, and high-throughput systems.

Key Features

1. Uses HTTP/2
2. Uses Protocol Buffers (Protobuf) for serialization
3. Strongly typed APIs
4. Supports bi-directional streaming
5. Cross-language support

gRPC Architecture

1. Client
2. Server
3. .proto file (service & message definitions)
4. Protobuf Compiler (protoc)
5. Generated client & server code

Protocol Buffers (Protobuf)

1. Binary serialization format
2. Smaller & faster than JSON
3. Schema-based

``` Example 

syntax = "proto3";

service UserService {
  rpc GetUser(UserRequest) returns (UserResponse);
}

message UserRequest {
  int32 id = 1;
}

message UserResponse {
  string name = 1;
}

-Communication Types in gRPC

1. Unary RPC
One request → One response

2. Server Streaming
One request → Stream of responses

3. Client Streaming
Stream of requests → One response

4. Bidirectional Streaming
Stream ↔ Stream

Why HTTP/2 Matters

1. Multiplexing (multiple requests over single connection)
2. Header compression
3. Lower latency
4. Better performance than HTTP/1.1

Advantages of gRPC

1. Very fast (binary + HTTP/2)
2. Strong contract (proto files)
3. Streaming support
4. Auto code generation
5. Ideal for internal microservices

Disadvantages of gRPC

1. Not human-readable (binary)
2. Browser support is limited
3. Harder to debug than REST
4. Overkill for simple CRUD APIs

RPC vs REST vs gRPC (Quick Comparison)
Feature	              REST	                  RPC	                    gRPC
Protocol	            HTTP	                  Any	                    HTTP/2
Data Format	          JSON	                  JSON/Binary	            Protobuf
Performance	          Medium	                Medium	                High
Streaming	            Limited	                Limited	                Excellent
Schema	              Weak	                  Medium	                Strong
Browser Friendly	    Yes	                    Yes	                    No (mostly)



















